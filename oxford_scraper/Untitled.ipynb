{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree, html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "\n",
    "\n",
    "source_url = 'https://www.oxfordlearnersdictionaries.com/definition/english/'\n",
    "\n",
    "# cool trick\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}\n",
    "\n",
    "my_list = [\"cushion\", \"trivial\", \"wanderer\"]\n",
    "my_words_dic = {}\n",
    "#source = requests.get(source_url, headers=headers).text\n",
    "for word in my_list:\n",
    "    my_dic = {}\n",
    "    try:\n",
    "        my_word_url = source_url + word\n",
    "        source = requests.get(my_word_url, headers=headers)\n",
    "        soup = BeautifulSoup(source.content.decode('utf-8'), 'lxml')\n",
    "        main_defs = soup.find_all('span', class_ = 'def')\n",
    "    except:\n",
    "        print(\"Bad Word...\")\n",
    "        \n",
    "    my_li = soup.find(id=\"cushion_sng_1\")\n",
    "    # selects the main text\n",
    "\n",
    "    main_examples = soup.find_all('span', class_ = 'x')\n",
    "    potential_image_links = soup.find_all('a', href=True)\n",
    "    #potential_image_links = soup.find_all('img')\n",
    "\n",
    "    # https://www.oxfordlearnersdictionaries.com/media/english/fullsize\n",
    "\n",
    "    for index, my_def in enumerate(main_defs):\n",
    "        my_dic[\"definition_\" + str(index+1)] = my_def.text\n",
    "        print(my_def.text)\n",
    "\n",
    "    for jndex, my_ex in enumerate(main_examples):\n",
    "        my_dic[\"example_set_\" + str(jndex+1)] = my_ex.text\n",
    "        print(my_ex.text)\n",
    "\n",
    "    for my_pic in potential_image_links:\n",
    "        if \"https://www.oxfordlearnersdictionaries.com/media/english/fullsize\" in my_pic[\"href\"]:\n",
    "            print(my_pic['href'])\n",
    "\n",
    "    my_words_dic[word] = my_dic\n",
    "    #print(len(main_defs))\n",
    "    #print(len(main_examples))\n",
    "    #print(len(potential_image_links))\n",
    "    print(my_li)\n",
    "\n",
    "for k, v in my_words_dic.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "\n",
    "#with open('ref_seshat.csv', 'w') as my_file:\n",
    "#    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree, html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "\n",
    "\n",
    "source_url = 'https://www.oxfordlearnersdictionaries.com/definition/english/'\n",
    "\n",
    "# cool trick\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}\n",
    "\n",
    "my_list = [\"wanderlust\", \"cushion\", \"wanderer\"]\n",
    "my_words_list_of_dics = []\n",
    "#source = requests.get(source_url, headers=headers).text\n",
    "for word in my_list:\n",
    "    my_dic_for_this_word = {\n",
    "        \"word\": word,\n",
    "        \"pos\": \"xyz\",\n",
    "        \"phon\": \"abc\",\n",
    "    }\n",
    "    for i in [1,2,3,4]:\n",
    "        my_word_url = source_url + word + \"_\" + str(i)\n",
    "        source = requests.get(my_word_url, headers=headers)\n",
    "        soup = BeautifulSoup(source.content.decode('utf-8'), 'lxml')\n",
    "        main_defs = soup.find_all('span', class_ = 'def')\n",
    "        \n",
    "        if not main_defs:\n",
    "            continue\n",
    "        \n",
    "        # parts of speech\n",
    "        try:\n",
    "            my_pos = soup.find('span', class_ = 'pos')\n",
    "            my_dic_for_this_word[\"pos\"] = my_pos.text\n",
    "        except:\n",
    "            my_dic_for_this_word[\"pos\"] = \"NoPOSFound\"\n",
    "\n",
    "        # phonetics\n",
    "        try:\n",
    "            my_american_pros = soup.find('div', class_=\"phons_n_am\")\n",
    "            my_american_pro = my_american_pros.find_all('span', class_ = 'phon')\n",
    "            my_dic_for_this_word[\"phon\"] = my_american_pro[0].text\n",
    "        except:\n",
    "            my_dic_for_this_word[\"phon\"] = \"NoProFound\"\n",
    "            \n",
    "        # ol with a class of sense_single or senses_multiple\n",
    "        try:\n",
    "            my_multiple_defs = soup.find('ol', class_=\"senses_multiple\")\n",
    "            # number of defs\n",
    "            main_defs = my_multiple_defs.find('span', class_ = 'def')\n",
    "            for index, my_def in enumerate(main_defs):\n",
    "                list_item_id = word + \"_sng_\" + str(index+1)\n",
    "                my_smaller_soup = my_def.find(id=list_item_id)\n",
    "                my_dic_for_this_word[\"def_\" + str(index+1)] = my_def.text\n",
    "                try:\n",
    "                    my_examples = my_smaller_soup.find_all('span', class_ = 'x')\n",
    "                    my_dic[\"example_set_\" + str(index+1)] = [my_ex.text for my_ex in my_examples]\n",
    "                except:\n",
    "                    my_dic[\"example_set_\" + str(index+1)] = \"NoExamples\"              \n",
    "        except:\n",
    "            my_single_def = soup.find('ol', class_=\"sense_single\")\n",
    "            list_item_id = word + \"_sng_1\"\n",
    "            my_smaller_soup = my_single_def.find(id=list_item_id)\n",
    "            # the only def:\n",
    "            main_def = my_smaller_soup.find('span', class_ = 'def')\n",
    "            my_dic_for_this_word[\"def_1\"] = main_def.text\n",
    "            # the only set of examples:\n",
    "            try:\n",
    "                main_examples = my_smaller_soup.find_all('span', class_ = 'x')\n",
    "                my_dic[\"example_set_1\"] = [my_ex.text for my_ex in main_examples]\n",
    "            except:\n",
    "                my_dic[\"example_set_1\"] = \"NoExamples\"\n",
    "        my_words_list_of_dics.append(my_dic_for_this_word)\n",
    "        \n",
    "\n",
    "\n",
    "        main_examples = my_smaller_soup.find_all('span', class_ = 'x')\n",
    "        \n",
    "        \n",
    "        # images\n",
    "        potential_image_links = soup.find_all('a', href=True)\n",
    "        #potential_image_links = soup.find_all('img')\n",
    "\n",
    "        for jndex, my_ex in enumerate(main_examples):\n",
    "            my_dic[\"example_set_\" + str(jndex+1)] = my_ex.text\n",
    "            print(my_ex.text)\n",
    "        print(my_american_pro[0].text)\n",
    "\n",
    "\n",
    "#with open('ref_seshat.csv', 'w') as my_file:\n",
    "#    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cushion_topg_3 > div > span.phonetics > div.phons_n_am > span\n",
    "#wanderer_topg_1 > div > span.phonetics > div.phons_n_am > span\n",
    "#wanderlust_topg_1 > div > span.phonetics > div.phons_n_am > span\n",
    "#take_topg_2 > div > span.phonetics > div.phons_br > span\n",
    "#take_topg_2 > div > span.phonetics > div.phons_n_am > span\n",
    "\n",
    "#take_topg_57 > div > span.phonetics > div.phons_n_am > span\n",
    "\n",
    "#wanderlust_topg_1 > div > span.phonetics > div.phons_n_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excessive-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going for word:  maroon 1\n",
      "fffffffff: 1\n",
      "WORD:  maroon_sng_1\n",
      "Going for word:  maroon 2\n",
      "fffffffff: 2\n",
      "WORD:  maroon_sng_2\n",
      "kkkk:  maroon_sng_2 noun\n",
      "examples Collected for:  maroon noun\n",
      "kkkk:  maroon_sng_2 noun\n",
      "examples Collected for:  maroon noun\n",
      "SUCCESS:  maroon noun\n",
      "Going for word:  maroon 3\n",
      "fffffffff: 1\n",
      "WORD:  maroon_sng_4\n",
      "Going for word:  maroon 4\n",
      "riddddiiiiiiiiiiiiii\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b1ccc7a98c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mmy_single_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ol'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sense_single\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mlist_item_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_sng_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdef_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mmy_smaller_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_single_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_item_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;31m# the only def:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m#print(my_smaller_soup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "from lxml import etree, html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "import json\n",
    "\n",
    "\n",
    "source_url = 'https://www.oxfordlearnersdictionaries.com/definition/english/'\n",
    "\n",
    "# cool trick\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}\n",
    "\n",
    "# my_list = [\"blip\",\"dichotomy\",\"loose\",\"loose\",\"cushion\",\"vain\",\"veil\",\"insurrection\",\"psychosis\",\"symbiosis\",\"complicity\",\"nebulous\",\"wince\",\"prudent\",\"haul\",\"frigid\",\"gawk\",\"trip\",\"oval\",\"elliptical\",\"ellipsis\",\"ellipse\",\"nudge\",\"hepatic\",\"vascular\",\"delirious\",\"delirium\",\"slur\",\"erratic\",\"bedridden\",\"untoward\",\"catheter\",\"maroon\",\"stalk\",\"tract\",\"sulk\",\"pet\",\"misogynist\",\"indulge\",\"hunch\",\"mascot\",\"badger\",\"ex-con\",\"thorough\",\"hone\",\"tamp\",\"tether\",\"confidant\",\"pertinent\",\"profanity\",\"hypnosis\",\"chime\",\"left field\",\"extradite\",\"fester\",\"fluff\",\"nonchalant\",\"vacillate\",\"odyssey\",\"undaunted\",\"headlong\",\"rung\",\"gesticulate\",\"precarious\",\"hots\",\"goose\",\"whittle\",\"fence\",\"crunch\",\"prong\",\"stoked\",\"presumptuous\",\"bat\",\"south\",\"rasp\",\"quack\",\"slowpoke\",\"upscale\",\"pyramid scheme\",\"goner\",\"progeny\",\"behemoth\",\"flustered\",\"trickle\",\"Homo sapiens\",\"chalet\",\"carrion\",\"unison\",\"trippy\",\"pew\",\"sullen\",\"discern\",\"prelude\",\"regurgitate\",\"righteous\",\"vehement\",\"revile\",\"inscrutable\",\"awash\",\"tick\",\"quackery\",\"stall\",\"mnemonic\",\"erogenous zone\",\"clairvoyant\",\"mucous membrane\",\"plumb\",\"plump\",\"plump\",\"harrowing\",\"jug\",\"flicker\",\"frugal\",\"cop\",\"streamer\",\"loot\",\"shroud\",\"squeamish\",\"acquiesce\",\"gregarious\",\"vanity\",\"indignation\",\"vertigo\",\"wistful\",\"hodge-podge\",\"bus\",\"pompous\",\"devolve\",\"slug\",\"tailspin\",\"callous\",\"bittersweet\",\"wrong\",\"innocuous\",\"scrunch\",\"conflagration\",\"flinch\",\"mealy\",\"groggy\",\"off-ramp\",\"catapult\",\"coax\",\"dabble\",\"banish\",\"posh\",\"obituary\",\"smother\",\"ballast\",\"rhapsody\",\"amenity\",\"blurb\",\"vortex\",\"flaunt\",\"arraign\",\"perk\",\"daunting\",\"crematorium\",\"slumber party\",\"fascism\",\"rink\",\"shambolic\",\"pneumonia\",\"heirloom\",\"quilt\",\"peachy\",\"retainer\",\"facade\",\"cellulite\",\"shenanigans\",\"flab\",\"antler\",\"zeitgeist\",\"scalp\",\"gonorrhea\",\"appease\",\"compartmentalize\",\"hunch\",\"occult\",\"flail\",\"demeaning\",\"humdrum\",\"juxtapose\",\"big time\",\"inundate\",\"meld\",\"date rape\",\"sleazy\",\"smooch\",\"bobby pin\",\"Q-tip\",\"grout\",\"squeegee\",\"stickler\",\"gravy\",\"diction\",\"piggyback\",\"mobilize\",\"labia\",\"Mardi Gras\",\"imbue\",\"charlie\",\"bloated\",\"detriment\",\"contingency\",\"rain check\",\"omen\",\"voodoo\",\"bail\",\"bleachers\",\"Sabbath\",\"slavish\",\"kudos\",\"lull\",\"shiftless\",\"pretentious\",\"rapier\",\"transfix\",\"distillery\",\"hiatus\",\"bonanza\",\"heroic\",\"mope\",\"shag\",\"filly\",\"gallivant\",\"droll\",\"fiendish\",\"cushy\",\"boulder\",\"attagirl\",\"mayhem\",\"hacksaw\",\"knackered\",\"outspoken\",\"inane\",\"seminal\",\"ruminate\",\"amends\",\"tacky\",\"conspicuous\",\"swoon\",\"bolster\",\"shanghai\",\"agape\",\"squint\",\"rabble-rousing\",\"grope\",\"festoon\",\"ostentation\",\"squirm\",\"prickly\",\"opulent\",\"to and fro\",\"zonked\",\"preposterous\",\"adrift\",\"small\",\"epiphany\",\"lenient\",\"hoax\",\"vernacular\",\"mung bean\",\"ottoman\",\"elope\",\"overbearing\",\"tumult\",\"behove\",\"bona fide\",\"spooky\",\"pedestal\",\"impasse\",\"third degree\",\"pounce\",\"contrived\",\"bulimia\",\"ingenious\",\"ebullient\",\"inhibited\",\"composite\",\"allegory\",\"thrifty\",\"immaculate\",\"blatant\",\"sage\",\"symbiotic\",\"intrusive\",\"primeval\",\"oversee\",\"grasping\",\"undertow\",\"virile\",\"tepid\",\"vagrant\",\"capricious\",\"shroud\",\"blanch\",\"linoleum\",\"splinter\",\"quid pro quo\",\"primp\",\"pact\",\"vent\",\"seethe\",\"hindsight\",\"cult\",\"venerate\",\"stash\",\"lint\",\"sizzle\",\"cult\",\"ruffle\",\"matter-of-fact\",\"head start\",\"proletariat\",\"bourgeoisie\",\"feather boa\",\"pry\",\"cocoon\",\"morbid\",\"squawk\",\"hump\",\"rugged\",\"hunky\",\"cue\",\"elf\",\"meadow\",\"muffled\",\"stoned\",\"smug\",\"stern\",\"ounce\",\"corpulent\",\"creep\",\"mutton chop whiskers\",\"drape\",\"sequel\",\"scribble\",\"glean\",\"grist\",\"conceit\",\"smart alec\",\"sorcerer\",\"doldrums\",\"lynch\",\"cringe\",\"glitch\",\"candid\",\"volition\",\"edgy\",\"verbatim\",\"self-indulgent\",\"chute\",\"nirvana\",\"withered\",\"haunch\",\"waddle\",\"snippet\",\"vignette\",\"furtive\",\"magnum opus\",\"elated\",\"absolve\",\"fondle\",\"bedlam\",\"refuge\",\"aesthetic\",\"luxuriate\",\"parody\",\"impassioned\",\"snub\",\"decadent\",\"avid\",\"estimable\",\"disparaging\",\"frenzy\",\"wayward\",\"jovial\",\"jocular\",\"burrow\",\"neck\",\"homely\",\"frizzy\",\"ostracize\",\"crouch\",\"bum\",\"crab\",\"taunt\",\"bubbly\",\"personable\",\"size\",\"stoop\",\"corroborate\",\"cordial\",\"alibi\",\"besmirch\",\"assignation\",\"thwart\",\"botch\",\"mishandle\",\"disbar\",\"rambling\",\"leak\",\"undignified\",\"memo\",\"player\",\"head\",\"hearty\",\"prosecutor\",\"chastise\",\"woodpecker\",\"petition\",\"technicality\",\"eunuch\",\"magnanimous\",\"lisp\",\"ham\",\"cynic\",\"alcove\",\"agnostic\",\"throb\",\"cream\",\"fraternize\",\"skid row\",\"caste\",\"emerald\",\"wallow\",\"albino\",\"smock\",\"reverie\",\"transom\",\"snatch\",\"fluke\",\"canvas\",\"rut\",\"deck\",\"glower\",\"dandy\",\"first-rate\",\"rabbit punch\",\"square\",\"prop\",\"infallible\",\"cannon\",\"tumble\",\"tuft\",\"salve\",\"forsake\",\"wad\",\"dole\",\"hoard\",\"pus\",\"loiter\",\"demerit\",\"gingham\",\"infer\",\"pylon\",\"mutter\",\"dab\",\"thing\",\"scrounge\",\"over\",\"sniff\",\"skewer\",\"born\",\"bandanna\",\"stuff\",\"beat\",\"alley\",\"jinx\",\"jinxed\",\"kiss-and-tell\",\"itchy\",\"along\",\"tipsy\",\"new\",\"reign\",\"couch potato\",\"grounded\",\"reverberate\",\"course\",\"purge\",\"knee-jerk\",\"covenant\",\"boilerplate\",\"duvet\",\"washcloth\",\"plunger\",\"toiletries\",\"hardwood\",\"windowpane\",\"pane\",\"sill\",\"rocking chair\",\"plush\",\"lounge\",\"knick-knack\",\"den\",\"colander\",\"condiment\",\"coaster\",\"stainless steel\",\"wok\",\"waxed paper\",\"parchment\",\"plastic wrap\",\"spatula\",\"paddle\",\"scraper\",\"ladle\",\"whisk\",\"garlic press\",\"pantry\",\"Tupperware\",\"serrated\",\"corkscrew\",\"tenderizer\",\"tongs\",\"tin\",\"contention\",\"sieve\",\"strainer\",\"SOB\",\"lurk\",\"huddle\",\"wee\",\"chickpea\",\"coon\",\"rat race\",\"thug\",\"agitator\",\"muzzle\",\"aphrodisiac\",\"enthuse\",\"pervasive\",\"bomb\",\"scrap\",\"interim\",\"thorny\",\"drab\",\"cloak\",\"nifty\",\"thrash\",\"pandemonium\",\"plummet\",\"renege\",\"behind\",\"pooh-pooh\",\"hell-bent\",\"come\",\"caddy\",\"twitch\",\"hitch\",\"butler\",\"MSG\",\"jaywalking\",\"toupée\",\"diaphragm\",\"scalp\",\"skimpy\",\"kibosh\",\"rustle\",\"baloney\",\"foul play\",\"faux pas\",\"shrink\",\"lustrous\",\"buff\",\"squeal\",\"emcee\",\"cuckoo\",\"berserk\",\"strain\",\"dunk\",\"busboy\",\"straw\",\"barricade\",\"seam\",\"stand-in\",\"grind\",\"straddle\",\"mitt\",\"scoop\",\"rendezvous\"]\n",
    "my_list = [\"maroon\"]\n",
    "my_words_list_of_dics = []\n",
    "#source = requests.get(source_url, headers=headers).text\n",
    "# the def_counter problem is solved. The other problem is with so many try and except blocks\n",
    "# I will rewrite everything with simple if and if else and it will be good.\n",
    "for word in my_list:\n",
    "    def_counter = 1\n",
    "    for i in [1,2,3,4]:\n",
    "        print(\"Going for word: \", word, str(i))\n",
    "        my_dic_for_this_word = {\n",
    "            \"word\": word,\n",
    "            \"pos\": \"xyz\",\n",
    "            \"phon\": \"abc\",\n",
    "            }\n",
    "        my_word_url = source_url + word + \"_\" + str(i)\n",
    "        #print(my_word_url)\n",
    "        source = requests.get(my_word_url, headers=headers)\n",
    "        soup = BeautifulSoup(source.content.decode('utf-8'), 'lxml')\n",
    "        main_defs = soup.find_all('span', class_ = 'def')\n",
    "        my_sng_situations = soup.find_all('li', {'id':re.compile(word + r'_sng_\\d{1,2}')})\n",
    "        #my_sng_situations = soup.find_all('li', {'id':re.compile(word + r'_sng_\\d{1,2}')})\n",
    "        if my_sng_situations:\n",
    "            print('fffffffff:', len(my_sng_situations))\n",
    "            a_potential_start_def_counter = my_sng_situations[0][\"id\"].split(\"_\")[-1]\n",
    "            print(\"WORD: \", my_sng_situations[0][\"id\"])\n",
    "            def_counter = a_potential_start_def_counter\n",
    "        else:\n",
    "            print(\"riddddiiiiiiiiiiiiii\")\n",
    "            pass\n",
    "        \n",
    "        # parts of speech\n",
    "        try:\n",
    "            my_pos = soup.find('span', class_ = 'pos')\n",
    "            my_dic_for_this_word[\"pos\"] = my_pos.text\n",
    "        except:\n",
    "            my_dic_for_this_word[\"pos\"] = \"NoPOSFound\"\n",
    "\n",
    "        # phonetics\n",
    "        try:\n",
    "            my_american_pros = soup.find('div', class_=\"phons_n_am\")\n",
    "            my_american_pro = my_american_pros.find_all('span', class_ = 'phon')\n",
    "            my_dic_for_this_word[\"phon\"] = my_american_pro[0].text\n",
    "        except:\n",
    "            my_dic_for_this_word[\"phon\"] = \"NoProFound\"\n",
    "            \n",
    "        # ol with a class of sense_single or senses_multiple\n",
    "        my_multiple_defs = soup.find('ol', class_=\"senses_multiple\")\n",
    "        my_single_def = soup.find('ol', class_=\"sense_single\")\n",
    "        if my_multiple_defs:\n",
    "            for index, my_def in enumerate(main_defs):\n",
    "                list_item_id = word + \"_sng_\" + str(def_counter)\n",
    "                try:\n",
    "                    my_smaller_soup_multi = my_multiple_defs.find(id=list_item_id)\n",
    "                    my_dic_for_this_word[\"def_\" + str(index+1)] = my_def.text\n",
    "                    def_counter = def_counter + 1\n",
    "                except:\n",
    "                    pass\n",
    "                    print(\"kkkk: \", list_item_id, my_dic_for_this_word[\"pos\"])\n",
    "                    #continue\n",
    "                try:\n",
    "                    #print(\"Haloooooo: \", list_item_id)\n",
    "                    my_examples = my_smaller_soup_multi.find_all('span', class_ = 'x')\n",
    "                    #print(my_smaller_soup_multi)\n",
    "                    my_dic_for_this_word[\"example_set_\" + str(index+1)] = [my_exm.text for my_exm in my_examples]\n",
    "                    print(\"examples Collected for: \", word, my_dic_for_this_word[\"pos\"])\n",
    "                except:\n",
    "                    my_dic_for_this_word[\"example_set_\" + str(index+1)] = \"NoExamples\"\n",
    "        else:\n",
    "            my_single_def = soup.find('ol', class_=\"sense_single\")\n",
    "            list_item_id = word + \"_sng_\" + str(def_counter)\n",
    "            my_smaller_soup = my_single_def.find(id=list_item_id)\n",
    "            # the only def:\n",
    "            #print(my_smaller_soup)\n",
    "            try:\n",
    "                main_def = my_smaller_soup.find('span', class_ = 'def')\n",
    "                my_dic_for_this_word[\"def_1\"] = main_def.text\n",
    "                def_counter = def_counter + 1\n",
    "            except:\n",
    "                continue\n",
    "            # the only set of examples:\n",
    "            try:\n",
    "                main_examples = my_smaller_soup.find_all('span', class_ = 'x')\n",
    "                #print(type(my_smaller_soup))\n",
    "                my_dic_for_this_word[\"example_set_1\"] = [my_ex.text for my_ex in main_examples]\n",
    "            except:\n",
    "                my_dic_for_this_word[\"example_set_1\"] = \"NoExamples\"\n",
    "        #else:\n",
    "        #    print(\"NoGOOD\")\n",
    "            #print(my_smaller_soup)\n",
    "        my_words_list_of_dics.append(my_dic_for_this_word)\n",
    "        print(\"SUCCESS: \", word, my_dic_for_this_word[\"pos\"])\n",
    "        \n",
    "    with open(\"my_json_file_of_oxford_test.json\", \"w\") as outfile:\n",
    "        json.dump(my_words_list_of_dics, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ambient-indicator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'maroon',\n",
       "  'pos': 'noun',\n",
       "  'phon': '/məˈruːn/',\n",
       "  'def_1': 'a dark red-brown colour',\n",
       "  'example_set_1': [],\n",
       "  'def_2': 'a large firework that shoots into the air and makes a loud noise, used to attract attention, especially at sea',\n",
       "  'example_set_2': []},\n",
       " {'word': 'maroon', 'pos': 'NoPOSFound', 'phon': 'NoProFound'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pet does not work properly because pet_sng_4 is the first id ... Weird\n",
    "\n",
    "my_words_list_of_dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_template = f\"\"\"\n",
    "\\newgeometry{{top=7mm, left=7mm, bottom=7mm, right=10mm}} \n",
    "\\pagecolor{{white}}\\afterpage{{\\nopagecolor}}\n",
    "\n",
    "\\begin{{singlespace}}\n",
    "\\fontsize{{80}}{{65}}\\selectfont \\textcolor{{FSBlue}}{{\\textbf{{{my_word} }}}} \\fontsize{{60}}{{65}}\\selectfont{{({my_pos})}}\n",
    "\n",
    "\\fontsize{{30}}{{45}}\\selectfont \\textcolor{{FSBlue}}{{\\textbf{{{my_phon}}}}\n",
    "\n",
    "\\fontsize{{40}}{{45}}\\selectfont\n",
    "\\textcolor{{FSBlue}}{{\\textbf{{{my_def}}}}}\n",
    "\n",
    "\\fontsize{{40}}{{65}}\\selectfont\n",
    "\\textcolor{{FSBlue}}{{{my_examples_well_separated}}}\n",
    "\n",
    "\\end{{singlespace}}\n",
    "\n",
    "\\thispagestyle{{empty}}\n",
    "\\restoregeometry   \n",
    "\\newpage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_big_list = []\n",
    "pos_converter = {\n",
    "    \"noun\": \"n\",\n",
    "    \"verb\": \"v\",\n",
    "    \"adjective\": \"adj\",\n",
    "    \"adverb\": \"adv\"\n",
    "}\n",
    "for item in my_words_list_of_dics:\n",
    "    \n",
    "    my_word = item[\"word\"]\n",
    "    my_pos = pos_converter.get(item[\"pos\"], item[\"pos\"])\n",
    "    my_phon = item[\"phon\"]\n",
    "    my_def = item.get(\"def_1\", \"NOTHING\")\n",
    "    my_examples = item.get(\"example_set_1\", [\"NOTHING\",])\n",
    "    my_examples_well_separated = \" ~~ \".join(my_examples)\n",
    "    my_template = f'''\n",
    "\\\\newgeometry{{top=10mm, left=10mm, bottom=10mm}} \n",
    "\\\\pagecolor{{white}}\\\\afterpage{{\\\\nopagecolor}}\n",
    "\n",
    "\\\\begin{{singlespace}}\n",
    "\\\\fontsize{{70}}{{65}}\\\\selectfont \\\\textcolor{{FSBlue}}{{\\\\textbf{{{my_word} }}}} \\\\fontsize{{50}}{{65}}\\\\selectfont{{({my_pos})}}\n",
    "\n",
    "\\\\fontsize{{25}}{{45}}\\\\selectfont \\\\textcolor{{FSBlue}}{{\\\\textbf{{{my_phon}}}}}\n",
    "\n",
    "\\\\fontsize{{40}}{{45}}\\\\selectfont\n",
    "\\\\textcolor{{FSBlue}}{{\\\\textbf{{{my_def}}}}}\n",
    "\n",
    "\\\\fontsize{{40}}{{65}}\\\\selectfont\n",
    "\\\\textcolor{{FSBlue}}{{{my_examples_well_separated}}}\n",
    "\n",
    "\\\\end{{singlespace}}\n",
    "\n",
    "\\\\thispagestyle{{empty}}\n",
    "\\\\restoregeometry   \n",
    "\\\\newpage\n",
    "'''\n",
    "    my_big_list.append(my_template)\n",
    "    \n",
    "my_big_string = \"\\n\".join(my_big_list)\n",
    "\n",
    "with open(\"big_string.text\", \"w\") as myfile:\n",
    "    myfile.write(my_big_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_converter = {\n",
    "    \"noun\": \"n\",\n",
    "    \"verb\": \"v\",\n",
    "    \"adjective\": \"adj\",\n",
    "    \"adverb\": \"adv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pos_converter.get(\"tr\", \"noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "constant-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \" abc def fdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dirty-faith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'def', 'fdf']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lstrip().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
